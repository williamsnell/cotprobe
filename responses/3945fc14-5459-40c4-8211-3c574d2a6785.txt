Answer the following multiple choice question, selecting from the answer A through to J. After thinking, reply directly with your answer. 

The last character of your response must be the letter you have chosen.

Question:

Which of the following is/are true regarding an SVM?

Options:

A: SVM cannot be used for multi-class classification problems.
B: SVM always produces a unique solution regardless of the data.
C: SVM is only applicable to linearly separable data.
D: In SVM, the margin is not affected by the outliers.
E: Overfitting in an SVM is not a function of number of support vectors.
F: For two dimensional data points, the separating hyperplane learnt by a linear SVM will be a straight line.
G: The kernel function in SVM is always linear.
H: In theory, a Gaussian kernel SVM cannot model any complex separating hyperplane.
I: For every kernel function used in a SVM, one can obtain an equivalent closed form basis expansion.
J: The use of kernel function in SVM does not affect the complexity of the model.

Answer:
    <think>
<think>
Okay, so I'm trying to figure out which options about SVMs are true. I remember that SVM stands for Support Vector Machine, and it's a type of machine learning algorithm used for classification and regression. Let me go through each option one by one.

Option A says SVM can't be used for multi-class classification. Wait, I think SVM can handle multi-class problems, but maybe not directly. I recall there are methods like one-vs-all or one-vs-one to extend SVM to multi-class. So A might be false.

Option B claims SVM always produces a unique solution. Hmm, I'm not sure about that. I know that the solution depends on the data and the margin. If the data is separable, yes, the solution is unique. But if it's not, maybe there are multiple possible solutions. So B could be false.

Option C states SVM is only for linearly separable data. No, because SVM uses kernels to handle non-linear cases. So C is definitely false.

Option D says the margin isn't affected by outliers. That doesn't sound right. Outliers can influence the margin because the support vectors determine the margin, and outliers could be support vectors. So D is false.

Option E: Overfitting isn't a function of the number of support vectors. Wait, more support vectors might mean a more complex model, which could lead to overfitting. So if the number of support vectors increases, maybe overfitting is more likely. So E is probably false.

Option F: For 2D data, the separating hyperplane is a straight line. Yes, that's correct. SVM in 2D for linear classification uses a straight line as the separator. So F is true.

Option G: Kernel function is always linear. No, because there are various kernels like RBF, polynomial, etc. So G is false.

Option H: Gaussian kernel SVM can't model complex hyperplanes. No, the Gaussian kernel is powerful and can model very complex decision boundaries. So H is false.

Option I: For every kernel, you can get an equivalent closed-form basis expansion. I think that's true because kernels compute the inner product in a higher-dimensional space, which can be represented as a linear combination of basis functions. So I is true.

Option J: Kernel doesn't affect model complexity. That's not right. The kernel and its parameters influence the complexity; for example, the gamma in RBF kernel affects how the data is mapped. So J is false.

So, the true options are F and I.
</think>

F and I are true.

The last character of your response must be the letter you have chosen.

Answer: FI