Answer the following multiple choice question, selecting from the answer A through to J. After thinking, reply directly with your answer. 

The last character of your response must be the letter you have chosen.

Question:

For the zero-memory information source described by the table, find the average length (L_avg) of binary code needed to encode the symbols. Source symbol Symbol probability P_i s_1 (1 / 4) s_2 (1 / 4) s_3 (1 / 4) s_4 (1 / 4)

Options:

A: 2 bits/symbol
B: 2.2 bits/symbol
C: 3.2 bits/symbol
D: 1.5 bits/symbol
E: 3 bits/symbol
F: 1.75 bits/symbol
G: 1 bit/symbol
H: 4 bits/symbol
I: 2.5 bits/symbol
J: 3.5 bits/symbol

Answer:
    
<think>
Okay, so I have this question about information theory, specifically about finding the average length of binary code needed to encode symbols from a zero-memory information source. Hmm, I'm a bit rusty on this, but let me think through it step by step.

Alright, the problem gives me four source symbols, s1 to s4, each with equal probabilities of 1/4. So the probabilities are the same for all symbols. I remember that in information theory, the average code length is related to entropy. Oh right, entropy is the expected value of the information content.

Wait, entropy is calculated as the sum of each symbol's probability multiplied by the logarithm (base 2) of its probability. So the formula is H = Î£ (P_i * log2(1/P_i)). Since all P_i are equal here, each P_i is 1/4.

Let me calculate the entropy first. Each symbol contributes (1/4) * log2(4), because 1/(1/4) is 4, and log2(4) is 2. So each term is (1/4)*2 = 0.5. Since there are four symbols, the total entropy H is 4 * 0.5 = 2 bits/symbol. So the entropy H is 2 bits/symbol.

Now, the question is about the average length of binary code needed. I think that relates to the concept of source coding, where we aim to find codes that have an average length close to the entropy. In this case, since each symbol has equal probability, maybe a fixed-length code would be optimal. Like, using 2 bits per symbol.

Wait, because each symbol has four possibilities, in binary you'd need two bits to represent four different values (00, 01, 10, 11). So each symbol can be represented with exactly two bits. That would make the average length L_avg equal to 2 bits/symbol.

Looking at the options, option A is 2 bits/symbol. So that seems like the correct answer.

But let me double-check. The entropy is 2, so the minimal average code length should be at least the entropy, right? Since entropy gives the lower bound. So if we can achieve exactly 2 bits, that's optimal. There's no need for a longer code on average because the entropy is 2.

I don't think any of the other options make sense. For example, option B is 2.2, which is higher than entropy but maybe that's for a different scenario. But here, with equal probabilities, we can have a fixed-length code, so 2 bits is achievable and optimal. So yeah, the average length is 2 bits/symbol.
</think>

The average length (L_avg) of binary code needed to encode the symbols can be determined using the entropy formula. Since each symbol has an equal probability of 1/4, the entropy H is calculated as 2 bits/symbol. Given that four symbols can be represented with two bits, a fixed-length code of 2 bits per symbol is optimal. Therefore, the average length is 2 bits/symbol.

Answer: A