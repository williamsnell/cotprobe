Answer the following multiple choice question, selecting from the answer A through to J. After thinking, reply directly with your answer. 

The last character of your response must be the letter you have chosen.

Question:

A train pulls out of the station at constant velocity. The received signal energy thus falls off with time as $1/i^2$. The total received signal at time $i$ is $Y_i = \frac{1}{i}X_i + Z_i$ where $Z_1, Z_2, \ldots$ are i.i.d. drawn from $N(0,1)$. The transmitter constraint for block length $n$ is $\frac{1}{n}\sum_{i=1}^n x_i^2(w) \leq 2  $ for $w \in \{1,2,\ldots, 2^{nR}\}$. Use Fano's inequality to find the capacity for this channel.

Options:

A: 1.5
B: 2.5
C: 1.0
D: 0.0
E: 0.5
F: 0.75
G: 3.0
H: 1.25
I: 0.25
J: 2.0

Answer:
    <think>
<think>
Okay, so I've got this question about channel capacity using Fano's inequality. Hmm, let's see what the problem says. 

A train is pulling out of the station at constant velocity, and the received signal energy falls off with time as 1/i². The received signal at time i is Y_i = (1/i)X_i + Z_i, where Z_i are i.i.d. drawn from N(0,1). The transmitter constraint for block length n is (1/n) sum from i=1 to n of x_i²(w) ≤ 2 for w in {1,2,...,2^{nR}}. I need to use Fano's inequality to find the capacity.

Wait, okay, Fano's inequality relates the conditional entropy H(X|Y) to the error probability P. It says H(X|Y) ≤ H(P) + P log(|Y|), where |Y| is the number of possible outcomes. But I'm not exactly sure how that applies here yet.

The channel is time-varying since the received signal strength decreases as 1/i. Each Y_i depends on X_i and Z_i, which is additive Gaussian noise. So the channel seems to be a Gaussian channel with time-varying gains (1/i) for each time slot i.

The capacity of a Gaussian channel is usually C = 0.5 * log(1 + SNR), but here SNR might vary over time. Wait, but since the SNR is varying, maybe we need to consider the time average or something else.

Wait, the question asks to use Fano's inequality. So maybe we're looking for an upper bound on the channel capacity. Fano's inequality can give a bound on the mutual information I(X;Y) in terms of the error probability. But I'm a bit confused about how to apply it in this context.

Alternatively, perhaps the channel is memoryless, but with varying gains each time. Wait, each Y_i is a separate observation, so maybe the channel is block-wise, but the SNR per time slot varies. Hmm.

Wait, the transmitter constraint is (1/n) sum x_i²(w) ≤ 2 for each w. So the average power per dimension is constrained. Wait, but if each x_i is multiplied by 1/i, then the effective power per symbol might be scaled.

Wait, perhaps the effective SNR at each time i is ( (1/i) x_i )² / 1, since Z_i is N(0,1). So the SNR for each Y_i is (x_i²)/(i²). Wait, because Y_i = (1/i) X_i + Z_i, so the variance of the noise is 1, so SNR is [ (1/i X_i)^2 ] / Var(Z_i) = (X_i²)/(i²) / 1 = X_i² / i².

But since the SNR varies with i, it's a time-varying channel. But how to compute the capacity then?

Wait, but the problem says to use Fano's inequality to find the capacity. So maybe we can model the channel's mutual information and bound it using Fano's.

Wait, Fano's inequality is H(X|Y) ≤ H(B) + B log(2), where B is the probability of error. But I'm not sure if that directly helps here.

Alternatively, for a Gaussian channel, the capacity is C = 0.5 log(1 + SNR_avg), but I'm not sure what the average SNR is here.

Wait, maybe we can think of each symbol being transmitted with power x_i², and the received signal is scaled by 1/i, so the effective power is x_i² / i². But the average power constraint is sum x_i² / n ≤ 2. So sum x_i² ≤ 2n.

But since each x_i is scaled by 1/i, the effective SNR for each symbol is (x_i² / i²) / 1 = x_i² / i². The total SNR per block would be the sum of these terms?

Wait, maybe the average SNR is sum (x_i² / i²) / n. But the average power is sum x_i² /n ≤ 2. So sum x_i² ≤ 2n.

But sum x_i² / i² would be less than or equal to sum x_i², but I'm not sure. Alternatively, perhaps the effective SNR per symbol is x_i²/(i²), so the average SNR across the block would be sum (x_i²/(i²)) /n.

Wait, but how does this relate to the capacity? For Gaussian channels, the capacity is the average of the log(1 + SNR) over the channel uses. So maybe C = (1/n) sum_{i=1}^n 0.5 log(1 + x_i^2/(i²)).

But since the average power is constrained to 2, we have sum x_i² ≤ 2n, so x_i^2 ≤ 2n for each i? No, that's not necessarily true because the sum is 2n, so individual terms could be up to 2n, but more likely, they're smaller.

Wait, but the problem is to find the capacity. Maybe the maximum is achieved when all x_i are equal, but considering the varying (1/i) scaling. Alternatively, perhaps the optimal strategy is to set x_i to maximize the mutual information under the power constraint.

Alternatively, perhaps using Fano's inequality, we can find a bound on the mutual information I(X;Y). Let me think: Fano's says H(X|Y) ≤ H(P_e) + P_e log |Y|, where P_e is the error probability. So mutual information I(X;Y) = H(X) - H(X|Y). If we can bound H(X|Y), then maybe we can bound I(X;Y).

But I'm not exactly sure how to proceed. Maybe another approach is better. Since the channel is Gaussian and time-varying, perhaps the capacity is the minimum of the capacities of each individual channel use, but that might not be accurate.

Wait, each Y_i is (1/i) X_i + Z_i, so each is a Gaussian channel with SNR of (X_i^2)/(i^2). The capacity for each is 0.5 log(1 + (X_i^2)/(i^2)). So the total mutual information would be the sum of these from i=1 to n.

So I(X;Y) = sum_{i=1}^n 0.5 log(1 + (X_i^2)/(i^2)).

We need to maximize this sum under the constraint that (1/n) sum X_i^2 ≤ 2.

Let me denote x_i = X_i. So the objective is to maximize sum_{i=1}^n 0.5 log(1 + x_i^2 / i^2), subject to (1/n) sum x_i^2 ≤ 2.

But perhaps it's easier to consider the problem in the limit as n approaches infinity, or perhaps find a lower bound.

Wait, but the options are specific numbers, so maybe the capacity is a fixed number regardless of n. Hmm.

Wait, another thought: if each x_i is scaled by 1/i, then the effective power per symbol is x_i^2 / i^2. The average of x_i^2 / i^2 over i from 1 to n is (1/n) sum x_i^2 / i^2. But the average power constraint is (1/n) sum x_i^2 ≤ 2. So the average of x_i^2 / i^2 would be less than or equal to 2 sum (1/i^2) over i from 1 to n. Wait, sum 1/i² converges to π²/6 ≈ 1.6449 for large n.

Wait, but I'm not sure if that's helpful.

Alternatively, perhaps the capacity is the minimum of the individual capacities. Because the channel's SNR decreases over time, the weakest link would be the last one (i=n), which has the smallest SNR. So perhaps the capacity is dominated by the last term, but that seems too simplistic.

Wait, but that might not be correct. Because each channel use is independent, so perhaps the total capacity is the sum of each channel's capacity. But each channel's capacity depends on x_i^2 / i^2, but we have a constraint on the average x_i^2.

Alternatively, perhaps the optimal x_i are chosen such that x_i^2 / i^2 is constant across all i. Let me think: if x_i^2 / i^2 = c for all i, then x_i = c i. Then the sum x_i^2 = c^2 sum i^2. The average power would be (c^2 sum i^2)/n ≤ 2. So c^2 ≤ 2n / sum i^2. For large n, sum i^2 ≈ n^3/3, so c^2 ≈ 6/(n^2). Then, each SNR is c^2 i² / i² = c². So each term in the mutual information would be 0.5 log(1 + c^2) = 0.5 log(1 + 6/(n^2)).

But as n increases, this approaches 0.5 log(1) = 0, which doesn't help. Hmm, maybe this approach isn't correct.

Wait, perhaps I should use Lagrange multipliers to maximize the mutual information under the constraint. Let me set up the function to maximize:

Objective: sum_{i=1}^n [0.5 log(1 + x_i^2 / i^2)]
Constraint: (1/n) sum x_i^2 ≤ 2.

Let me form the Lagrangian:

L = sum [0.5 log(1 + x_i^2/i^2)] - λ ( (1/n) sum x_i^2 - 2 )

Take derivative with respect to x_i:

dL/dx_i = [0.5 * (2 x_i / i^2) / (1 + x_i^2 / i^2) ] - λ (x_i / n) = 0.

Simplify:

( x_i / i^2 ) / (1 + x_i^2 / i^2) = λ x_i / n.

Assuming x_i ≠ 0, we can cancel x_i:

1/(i^2 (1 + x_i^2 / i^2)) ) = λ / n.

Multiply both sides by i^2 (1 + x_i^2 / i^2):

1 = λ i^2 (1 + x_i^2 / i^2) / n.

Hmm, but this seems complicated. Maybe all x_i are set to zero except for one i, but that might not maximize the mutual information.

Alternatively, perhaps the optimal strategy is to concentrate power in the earlier time slots where 1/i is larger, thus getting a higher SNR for those symbols. So perhaps x_i is larger for smaller i.

Wait, but I'm not sure. Alternatively, perhaps the capacity is zero because the SNR goes to zero as i increases, but that doesn't make sense because the average power is fixed.

Wait, another approach: since each Y_i is a separate observation, the overall channel is a product channel. The capacity would be the sum of the capacities of each individual channel. But each individual channel has an SNR of (X_i^2)/(i^2), and the average power constraint is E[X_i^2] = (1/n) sum x_i^2 ≤ 2.

Wait, but perhaps the average SNR across all channel uses is E[SNR_i] = E[X_i^2]/i^2. So the average mutual information per channel use would be 0.5 log(1 + E[SNR_i]).

But I'm not sure if that's the right approach.

Wait, maybe we can consider the time-averaged SNR. The average SNR would be sum_{i=1}^n (X_i^2 / i^2) / n. Under the constraint that sum X_i^2 ≤ 2n, the maximum of sum X_i^2 / i^2 would be when X_i is allocated as much as possible to the terms with smaller i, since 1/i^2 is larger for smaller i.

Wait, but the maximum sum of X_i^2 / i^2 under sum X_i^2 ≤ 2n would be achieved when X_i is assigned to the earliest terms. For example, allocate all power to X_1, then X_1^2 = 2n, and X_i=0 for i>1. Then sum X_i^2 / i^2 = (2n) / 1^2 = 2n. The average would be 2n / n = 2.

So the average SNR would be 2, leading to capacity per channel use of 0.5 log(1 + 2) = 0.5 log(3) ≈ 0.748. But since there are n channel uses, the total mutual information would be n * 0.748, but the capacity is usually per channel use, so perhaps 0.748, which is approximately 0.75. Looking at the options, F is 0.75.

Wait, but I'm not sure if that's correct because the SNR varies across channel uses. Alternatively, perhaps the capacity is the minimal SNR across the channel uses, but that might not be the case.

Wait, another thought: if we have a varying SNR, the capacity of the overall channel is the sum of the individual capacities. But each individual capacity is 0.5 log(1 + SNR_i), where SNR_i = x_i^2 / i^2. But under the average power constraint, perhaps the maximum total mutual information is achieved when each SNR_i is the same for all i. Let's assume SNR_i = SNR for all i. Then x_i^2 = SNR * i^2. The sum x_i^2 = SNR sum i^2 ≤ 2n. So SNR ≤ 2n / sum i^2.

For large n, sum i^2 ≈ n^3/3, so SNR ≤ 6/(n^2). Then each term in the mutual information is 0.5 log(1 + 6/(n^2)), which approaches 0 as n grows. That can't be right, so maybe this approach isn't valid.

Hmm, perhaps I'm overcomplicating this. Let's think about Fano's inequality again. Fano's gives a lower bound on the mutual information in terms of the conditional entropy. But I'm not sure how to apply it here without more information about the code or the error probability.

Alternatively, maybe the channel is such that the SNR decreases as 1/i², but the average power is fixed. So perhaps the effective SNR is scaled by 1/i², and the capacity is limited by the worst SNR.

Wait, but the problem says to use Fano's inequality, so perhaps the answer is related to the average power constraint leading to a certain SNR.

Wait, the average power is 2, so the average SNR would be 2, leading to a capacity of 0.5 log(1 + 2) = 0.5 log(3) ≈ 0.748, which is about 0.75. So the answer might be F: 0.75.

But I'm not entirely confident. Another approach: the capacity of a Gaussian channel with average power P is C = 0.5 log(1 + P). Wait, but in this case, the average power is 2, so C = 0.5 log(3) ≈ 0.748, which is 0.75. So that would make option F the correct answer.
</think>

The problem involves determining the capacity of a time-varying Gaussian channel using Fano's inequality. The received signal at time $i$ is given by $Y_i = \frac{1}{i}X_i + Z_i$, where $Z_i \sim N(0,1)$, and the transmitter constraint is $\frac{1}{n}\sum_{i=1}^n x_i^2 \leq 2$. The goal is to find the channel capacity.

The channel is Gaussian with a time-varying SNR, where the SNR at each time $i$ is $\text{SNR}_i = \frac{X_i^2}{i^2}$. The capacity of a Gaussian channel is typically $C = 0.5 \log(1 + \text{SNR})$. However, since the SNR varies with time, the total capacity is the sum of the individual capacities across all channel uses.

To maximize the mutual information under the power constraint, the optimal allocation of power is to concentrate it in the earlier time slots where $\frac{1}{i}$ is larger, leading to higher SNR. Assuming the average SNR is 2 (derived from the average power constraint), the capacity per channel use is approximately $0.5 \log(1 + 2) = 0.5 \log(3) \approx 0.748$, which rounds to $0.75$. This matches option F.

Answer: F