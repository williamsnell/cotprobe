Answer the following multiple choice question, selecting from the answer A through to J. After thinking, reply directly with your answer. 

The last character of your response must be the letter you have chosen.

Question:

Suppose that $(X, Y, Z)$ are jointly Gaussian and that $X \rightarrow Y \rightarrow Z$ forms a Markov chain. Let $X$ and $Y$ have correlation coefficient 0.1 and let $Y$ and $Z$ have correlation coefficient 0.9. Find $I(X;Z)$ in bits.

Options:

A: 0.45
B: 1.2
C: 0.1
D: 0.9
E: 0.00587
F: 0.15
G: 0.6
H: 0.7
I: 0.025
J: 0.3

Answer:
    <think>
<think>
Okay, so I've got this problem here about Gaussian variables and mutual information. Hmm, I'm a bit rusty on this, but I'll try to think it through.

The question says that X, Y, Z are jointly Gaussian, forming a Markov chain: X → Y → Z. So that means Y is dependent on X, and Z is dependent on Y. Also, the correlation between X and Y is 0.1, and between Y and Z is 0.9. I need to find I(X; Z), the mutual information between X and Z.

Wait, mutual information I(X; Z) is a measure of how much knowing X tells us about Z, or vice versa. For jointly Gaussian variables, there's a formula involving the standard deviations and the correlation.

Since X, Y, Z are Gaussian and form a chain, maybe I can model them as such. The mutual information between X and Z can be found using the formula I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z). But wait, since Y is in between X and Z, and they form a Markov chain, I think that the mutual information from X to Z is just the sum of the mutual information between X and Y plus between Y and Z, minus the mutual information between X and Z through Y, but wait, no, because if X and Z are conditionally independent given Y, then I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z). But if X and Z are independent given Y, then I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z). But I'm not sure about the exact formula here.

Alternatively, for jointly Gaussian variables, mutual information can be computed using the formula involving the correlation coefficients. Since X, Y, Z are Gaussian, I(X; Z) can be found by the formula I(X; Z) = 0.5 * log( (1 + ρ_XZ)/(1 - ρ_XY)(1 - ρ_YZ) ) ), but wait, am I getting that right?

Wait, another approach: mutual information between X and Z is equal to the mutual information from X to Y plus the mutual information from Y to Z minus the mutual information from X to Z through Y, but since in a Markov chain, X and Z are conditionally independent given Y, the I(X; Z) is equal to I(X; Y) + I(Y; Z) - I(X; Y; Z). But because X and Z are independent given Y, then I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z). But wait, if X and Z are independent given Y, then I(X; Y; Z) is zero? Or maybe that's not the case.

Wait, perhaps I should think in terms of conditional entropies. Since X and Z are conditionally independent given Y, the mutual information I(X; Z) is equal to I(X; Y) + I(Z; Y) minus I(X; Z; Y), but since they're conditionally independent, I(X; Z; Y) is just I(X; Y) + I(Z; Y). Hmm, maybe I'm getting stuck here.

Alternatively, for jointly Gaussian variables, mutual information can be found using the formula:

I(X; Z) = 0.5 * log( (1 + ρ_XZ) / (1 - ρ_XY)(1 - ρ_YZ) ) )

Wait, but that seems like the formula for mutual information when X, Y, Z are univariate Gaussians with known correlations. But I'm not sure if that's correct because I think mutual information isn't just a function of pairwise correlations. Oh wait, but for Gaussian variables, mutual information is a function of the correlations. 

Wait, no, mutual information for jointly Gaussian variables is not directly a function of the pairwise correlations because it's a measure of dependence, not just correlation. So maybe that formula isn't applicable here.

Another approach: since X, Y, Z are Gaussian, the mutual information I(X; Z) can be found using the formula:

I(X; Z) = 0.5 * log( (1 + ρ_XZ) / (1 - ρ_XY)(1 - ρ_YZ) ) )

Wait, but in this case, I'm not sure about the exact formula. Alternatively, for Gaussians, the mutual information can be written in terms of the entropy of Z given X, but I'm not recalling the exact expression.

Wait, perhaps I should model the variables with their joint distribution. Let's say X, Y, Z are jointly Gaussian. Let me denote the correlation coefficients as ρ_XY = 0.1 and ρ_YZ = 0.9. What about ρ_XZ? Since the chain is X→Y→Z, the correlation between X and Z can be found as ρ_XZ = ρ_XY * ρ_YZ + ρ_XY * ρ_YZ? No, wait, that's not right. The correlation between X and Z can be found using the formula:

ρ_XZ = ρ_XY * ρ_YZ + (1 - ρ_XY^2)(1 - ρ_YZ^2)^0.5 * ρ_XY * ρ_YZ ... Wait, I'm getting confused. Maybe it's better to compute it using the chain rule for mutual information.

Wait, mutual information between X and Z is I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z). But since Y is in between X and Z, and the chain is Markov, the mutual information I(X; Y; Z) is I(X; Y) + I(Y; Z) - I(X; Z), which doesn't make sense. Alternatively, perhaps I(X; Y; Z) is zero if Y is conditionally independent of X given Z or something. Wait, no, because in the chain X→Y→Z, Y is already determined by X, and Z is determined by Y. So if Y is known, X is determined, so Y and X are not independent once Y is known. So perhaps I(X; Y; Z) is zero, but I'm not sure.

Alternatively, perhaps I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z) is the formula. But I'm getting stuck. Maybe I should think in terms of conditional entropies.

I(X; Z) = H(Z) - H(Z | X)

Similarly, I(X; Z) can also be computed as H(Z) - H(Z | X), but that doesn't directly help.

Alternatively, since X, Y, Z are Gaussian, the joint distribution is a multivariate Gaussian, and I(X; Z) can be calculated using the formula for Gaussian mutual information.

Wait, I think mutual information for jointly Gaussian variables is given by:

I(X; Z) = 0.5 * log( (1 + ρ_XZ) / (1 - ρ_XY)(1 - ρ_YZ) ) )

Wait, I'm not certain about this formula. Alternatively, I've heard that mutual information for jointly Gaussian variables is a function of the correlation between X and Z, but also the correlations with Y.

Alternatively, another formula for mutual information between X and Z is:

I(X; Z) = 0.5 * log( [ (1 + ρ_XZ) / (1 - ρ_XY)(1 - ρ_YZ) ] )

But I'm not sure if that's the correct formula. Let me think: perhaps I should compute the conditional entropy H(Z | X) and then subtract it from H(Z) to get I(X; Z).

Since X, Y, Z are Gaussian, H(Z | X) is determined by the conditional distribution of Z given X. For a Gaussian Markov chain, the conditional distribution of Z given X is also Gaussian, with mean μ_Z|X and variance σ²_Z|X.

Let me model this. Let's say X, Y, Z are all Gaussian random variables, with zero mean for simplicity (since the problem doesn't specify any means; I can assume they are centered).

X → Y → Z implies that X is Gaussian, and Y is Gaussian with mean 0, and Z is Gaussian with mean 0. The correlation between X and Y is 0.1, and between Y and Z is 0.9.

Let me denote the standard deviations as σ_X, σ_Y, σ_Z. I can assume they are all 1 for simplicity, unless stated otherwise, because Gaussian variables can be scaled.

Wait, but I'm not sure about that. Wait, maybe I should think about the joint distribution. Since X and Y are Gaussian, and Y and Z are Gaussian, and X, Y, Z are jointly Gaussian, the chain is X → Y → Z.

In a Gaussian Markov chain, the conditional distribution of Z given X is Gaussian. The formula for the conditional mean and variance can be derived.

The conditional mean of Z given X is μ_Z|X = ρ_YZ * σ_Z * (X - μ_X) / σ_X. Since X is zero-mean, this simplifies to μ_Z|X = ρ_YZ * σ_Z * X / σ_X.

Similarly, the conditional variance of Z given X is σ²_Z|X = σ²_Z (1 - ρ_YZ²).

Now, the mutual information I(X; Z) can be found using the formula:

I(X; Z) = H(Z) - H(Z | X)

But since Z is Gaussian, H(Z) is 0.5 log(2πσ_Z²) + 0.5 σ_Z² log(2π).

H(Z | X) is 0.5 log(2πσ_Z|X²) + 0.5 σ_Z|X² log(2π). Let's compute that.

H(Z | X) = 0.5 log(2π) + 0.5 (σ_Z² (1 - ρ_YZ²)) log(2π) + 0.5 (ρ_YZ² σ_Z²) [since (σ_Z|X)^2 is σ_Z² (1 - ρ_YZ²), so (σ_Z|X)^2 log(2π) is σ_Z² (1 - ρ_YZ²) * 0.5 log(2π) ? Wait, no, H(Z | X) is the entropy of Z given X, which is:

H(Z | X) = 0.5 log(2π) + 0.5 log(σ_Z² (1 - ρ_YZ²)) + 0.5 σ_Z² (1 - ρ_YZ²) log(2π) ?

Wait, I'm getting a bit confused. Let me think again. The conditional distribution of Z given X is Gaussian with mean μ_Z|X = ρ_YZ σ_Z / σ_X X (assuming X is standard, so σ_X=1). The variance is σ²_Z (1 - ρ_YZ²). So the entropy H(Z | X) for a Gaussian variable is:

H(Z | X) = 0.5 log{ 2π σ_Z² (1 - ρ_YZ²) / σ_Z² } } 

Wait, no: the entropy of a Gaussian is 0.5 log(2πσ²) + 0.5 log(σ²). So H(Z | X) is 0.5 log(2π) + 0.5 log(σ_Z² (1 - ρ_YZ²)).

So H(Z | X) = 0.5 log(2π) + 0.5 log(σ_Z² (1 - ρ_YZ²)).

Thus, H(Z | X) is 0.5 log(2π) + 0.5 log(σ_Z² (1 - ρ_YZ²)).

But H(Z) is 0.5 log(2πσ_Z²) + 0.5 σ_Z² log(2π).

So I(X; Z) = H(Z) - H(Z | X) = [0.5 log(2πσ_Z²) + 0.5 σ_Z² log(2π)] - [0.5 log(2π) + 0.5 log(σ_Z² (1 - ρ_YZ²)) ]

Simplify this:

= 0.5 log(2πσ_Z²) + 0.5 σ_Z² log(2π) - 0.5 log(2π) - 0.5 log(σ_Z² (1 - ρ_YZ²))

= 0.5 log(2πσ_Z²) - 0.5 log(σ_Z² (1 - ρ_YZ²)) )

= 0.5 [ log(2πσ_Z²) - log(σ_Z² (1 - ρ_YZ²)) ) ]

= 0.5 [ log( (2πσ_Z²) / (σ_Z² (1 - ρ_YZ²)) ) ) ]

= 0.5 [ log( (2π) / (1 - ρ_YZ²) ) ) ]

Wait, because σ_Z² cancels out.

So I(X; Z) = 0.5 log( (2π) / (1 - ρ_YZ²) ) )

Wait, but wait, that doesn't seem to involve ρ_XY. Because the mutual information between X and Z depends only on the correlation between Y and Z, not directly on X and Y's correlation.

Hmm, that seems odd. Because in a Markov chain X→Y→Z, the information flow from X to Z passes through Y, so the mutual information I(X; Z) should involve the correlation between X and Y, as well as Y and Z.

Wait, maybe my approach is wrong because I assumed that I can compute H(Z|X) without considering X's correlation with Y.

Wait, perhaps I should model this differently.

Since X, Y, Z are jointly Gaussian, the joint distribution is a multivariate normal distribution. The mutual information between X and Z can be calculated using the formula involving the correlation between X and Z, but also the correlations between X and Y, and Y and Z.

Wait, maybe the mutual information I(X; Z) is given by:

I(X; Z) = 0.5 log( (1 + ρ_XZ) / ( (1 - ρ_XY)(1 - ρ_YZ) )) )

But I'm not sure. Alternatively, perhaps I should compute the conditional entropy H(Z | X) and then find I(X; Z) = H(Z) - H(Z | X).

Wait, let's plug in the values.

We have:

ρ_YZ = 0.9.

So in the formula, I(X; Z) = 0.5 * log( (1 + ρ_XZ) / ( (1 - ρ_XY)(1 - ρ_YZ) ) ) )

But wait, without knowing ρ_XZ, this would be impossible to compute. Hmm, maybe I'm missing something.

Wait, but wait: Since Y is in the middle of X and Z, and the chain is Markov, perhaps the mutual information I(X; Z) can be expressed using the formula for conditional mutual information. Alternatively, perhaps I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z).

But in this scenario, since X → Y → Z is a Markov chain, I(X; Y; Z) = I(X; Y) + I(Y; Z) - I(X; Z) ?

Wait, that can't be right. Alternatively, I think I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z). Since X and Z are conditionally independent given Y, I(X; Y; Z) = I(X; Y) + I(Z; Y) - I(X; Y; Z). Hmm, maybe I should use the formula for conditional entropy.

Wait, perhaps I need to find the correlation between X and Z. How?

Since X → Y → Z, and Y is Gaussian, then X and Z are Gaussian variables as well. Let me compute the covariance between X and Z.

Wait, but since Y is in between, the covariance between X and Z can be written as Cov(X, Z) = Cov(X, E[Z|X]) + Cov(E[X|Y], Z). But since X and Y are not independent, this might get complicated.

Alternatively, since Y is Gaussian, perhaps I can express the joint distribution in terms of Y. Let me think in terms of the joint distribution.

But perhaps a better approach is to model the variables as follows:

Let’s denote X as a Gaussian variable with mean 0 and variance σ_X². Y is Gaussian with mean 0, variance σ_Y², and correlation ρ_XY = 0.1 with X. Z is Gaussian with mean 0, variance σ_Z², and correlation ρ_YZ = 0.9 with Y.

But since X → Y → Z is a Markov chain, the conditional distribution of Z given X is Gaussian, with mean μ_Z|X = ρ_YZ * σ_Z * X / σ_X, and variance σ²_Z|X = σ_Z² (1 - ρ_YZ²).

But without knowing σ_X, σ_Y, σ_Z, perhaps I can assume they are all 1 for simplicity, because the problem doesn’t specify any other parameters. So let's set σ_X = σ_Y = σ_Z = 1.

Then, Cov(X, Z) = E[X Z] = E[ X * (ρ_YZ X + ε) ], where ε is the error term with variance σ²_Y (1 - ρ_YZ²). So Cov(X, Z) = ρ_YZ * E[X²] = ρ_YZ * σ_X² = 0.9 * 1² = 0.9.

So the correlation between X and Z is 0.9. Wait, but that's not correct because if X and Y are correlated, and Y and Z are correlated, then X and Z have a correlation that's the product of the two correlations, or perhaps something else.

Wait, I think that's the case, but let me compute it properly. Let me model X, Y, Z as Gaussian variables. Let X and Y have correlation ρ_XY = 0.1. Y and Z have correlation ρ_YZ = 0.9. We're assuming X, Y, Z are jointly Gaussian.

The covariance between X and Z is Cov(X, Z) = E[X Z] = Cov(X, E[Z|X]) + Cov(E[X|Y], Z). Since X and Y are not independent, E[X|Y] is a random variable.

But since Y is a Gaussian variable with mean 0 and variance 1, and X is a Gaussian variable with mean 0 and variance 1, and they have a correlation ρ_XY = 0.1, X is a linear function of Y plus a Gaussian error.

Wait, perhaps a better approach is to model X, Y, Z as a chain where X is a Gaussian variable, Y is a Gaussian variable with mean 0 and variance 1, and X is correlated with Y by 0.1. Then, Z is a Gaussian variable with mean 0, variance 1, and correlated with Y by 0.9.

But then, to model the joint distribution, we can write:

Y = Z + (X - Y) * ρ_XY / σ_X

Wait, perhaps it's getting too complicated. Alternatively, perhaps I can model the variables as:

X → Y → Z

where X is zero-mean Gaussian, Y is zero-mean Gaussian with variance 1, and X is correlated with Y by 0.1. Then Z is zero-mean Gaussian with variance 1, and is correlated with Y by 0.9.

Wait, but when you have a chain like X → Y → Z, and all variables are Gaussian, the joint distribution is a Gaussian with mean zero and covariance matrix:

[ [1, 0.1, 0.9 ],
  [0.1, 1, 0.9 ],
  [0.9, 0.9, 1 ] ]

Wait, is that correct? Because the covariance matrix for X, Y, Z would have:

- Cov(X, Y) = 0.1
- Cov(X, Z) = 0.9
- Cov(Y, Z) = 0.9
- Cov(X, X) = 1
- Cov(Y, Y) = 1
- Cov(Z, Z) = 1

So the covariance matrix is:

[[1, 0.1, 0.9],
[0.1, 1, 0.9],
[0.9, 0.9, 1]]]

Wait, that can't be right. Because in a Markov chain X→Y→Z, the covariance between X and Z is equal to the covariance between X and Y multiplied by the covariance between Y and Z, but that's only true if Y is a Gaussian variable with mean 0 and variance 1. Hmm, perhaps that's the case.

Wait, another approach: the joint distribution of X, Y, Z can be written as a Gaussian with mean zero, and covariance matrix C. Since X, Y, Z are Gaussian, the joint distribution is a multivariate normal distribution.

The covariance matrix C would have:

C[1, 1] = 1 (X is scaled to zero mean)
C[2, 2] = 1 (Y is scaled)
C[3, 3] = 1 (Z is scaled)

C[1, 2] = 0.1 (X and Y are correlated)
C[1, 3] = 0.9 (X and Z are correlated)
C[2, 3] = 0.9 (Y and Z are correlated)

So the covariance matrix is:

[1, 0.1, 0.9],
[0.1, 1, 0.9],
[0.9, 0.9, 1]]]

Wait, that seems correct. Now, I can compute the mutual information I(X; Z) using the formula for multivariate Gaussian mutual information.

Wait, the formula for mutual information in a multivariate Gaussian distribution is:

I(X; Z) = 0.5 * [ log( (2π)^3 σ^3 ) / σ^3 ) ] - 0.5 * [ log( (2π)^3 product of variances ) / product of variances ) ]

Wait, maybe I should compute it using the entropy formula. Since I(X; Z) = H(Z) - H(Z | X), and H(Z) is the entropy of a Gaussian variable with variance 1, which is 0.5 log(2π) + 0.5. Hmm, but that's just a constant, so I(X; Z) would depend on the conditional entropy H(Z | X).

Wait, but using the formula from information theory, the mutual information between two Gaussian variables X and Z is given by:

I(X; Z) = 0.5 * log( (1 + ρ_XZ) / ( (1 - ρ_XY)(1 - ρ_YZ) ) ) )

But in this case, if I'm considering X and Z as a joint Gaussian, then ρ_XZ is the correlation between X and Z. But how is ρ_XZ related to the other correlations?

Wait, perhaps I should compute ρ_XZ using the chain rule. Since X → Y → Z is a Markov chain, we have:

ρ_XZ = ρ_XY * ρ_YZ + (1 - ρ_XY^2) * (1 - ρ_YZ^2) * ρ_XY * ρ_YZ 

Wait, maybe not. Let me think: For X and Z, given that Y is in between, the correlation between X and Z can be computed using the formula:

ρ_XZ = ρ_XY * ρ_YZ + (1 - ρ_XY^2) * (1 - ρ_YZ^2) ) * ρ_XY * ρ_YZ 

Wait, no. That would be for a case where Y is a Gaussian variable, and X and Z are dependent only through Y. So the formula for ρ_XZ is:

ρ_XZ = ρ_XY * ρ_YZ + (1 - ρ_YZ^2) * (1 - ρ_XY^2) ) * ρ_XY * ρ_YZ 

Wait, I'm not confident about that. Alternatively, perhaps the formula is:

ρ_XZ = ρ_XY * ρ_YZ + (1 - ρ_YZ^2) * (1 - ρ_XY^2) ) * (1 - ρ_XY * ρ_YZ )

But I'm not certain.

Wait, perhaps a better approach is to consider that in a Markov chain X→Y→Z, the conditional distribution of Z given X is Gaussian, with mean μ_Z|X and variance σ²_Z|X. So:

μ_Z|X = ρ_YZ * σ_Z / σ_X * X

Var(Z | X) = σ_Z² (1 - ρ_YZ²)

Now, H(Z | X) is the entropy of a Gaussian variable with mean μ_Z|X and variance σ²_Z|X.

H(Z | X) = 0.5 * log(2π) + 0.5 * log(σ²_Z|X) 

But since X is Gaussian, and ρ_YZ is given, and σ_Z is 1 (assuming unit variance), then:

H(Z | X) = 0.5 * log(2π) + 0.5 * log( (1 - ρ_YZ^2) )

Now, H(Z) is the entropy of Z, which is 0.5 * log(2π) + 0.5 * log(1) = 0.5 * log(2π) + 0.5 * log(1), but that's just a constant. Wait, no, because Z has mean 0, but when considering entropy, the variance is 1.

Wait, perhaps I should compute the mutual information I(X; Z) using the formula:

I(X; Z) = H(Z) - H(Z | X)

But H(Z) is the entropy of Z, which is 0.5 * log(2π) + 0.5 * log(1) = 0.5 * log(2π) + 0.5. H(Z | X) is 0.5 * log(2π) + 0.5 * log(1 - ρ_YZ^2). 

So subtracting:

I(X; Z) = [0.5 * log(2π) + 0.5] - [0.5 * log(2π) + 0.5 * log(1 - ρ_YZ^2)]

Simplifies to:

I(X; Z) = 0.5 - 0.5 * log(1 - ρ_YZ²)

Wait, that makes sense because as ρ_YZ increases, the mutual information increases, which is consistent with the problem statement.

But wait, let's plug in the given ρ_YZ = 0.9.

I(X; Z) = 0.5 - 0.5 * log(1 - 0.81) = 0.5 - 0.5 * log(0.19) ≈ 0.5 - 0.5 * (-1.714) ≈ 0.5 + 0.857 ≈ 1.357 bits.

Wait, but the options given are A: 0.3, B:1.2, C:2.5, D:0.25.

Wait, but according to this, the mutual information I(X; Z) is approximately 1.357 bits. But looking at the options, the closest is B: 1.2.

Hmm, but wait, perhaps I made a mistake in the formula. Let me double-check.

Wait, the formula I derived is I(X; Z) = 0.5 - 0.5 * log(1 - ρ_YZ²). Let's compute this for ρ_YZ = 0.9.

Compute 1 - ρ_YZ² = 1 - 0.81 = 0.19.

log(0.19) is approximately -1.714.

So 0.5 - 0.5*(-1.714) = 0.5 + 0.857 = 1.357 bits.

Hmm, but the options don't have 1.357. The closest is 1.2 (option B). So perhaps the correct answer is B:1.2.

Wait, but let's think again. The mutual information I(X; Z) = 0.5 - 0.5 * log(1 - ρ_YZ²). Let me compute this for ρ_YZ = 0.9.

log(0.19) is indeed approximately -1.714.

So 0.5 - 0.5*(-1.714) = 0.5 + 0.857 = 1.357.

But none of the options is 1.357. The closest is 1.2 (option B). So perhaps the answer is B:1.2.

Wait, but another possibility is that I made a mistake in the formula, and the correct mutual information is 0.25, which is option D. But I'm not sure.

Alternatively, perhaps I should recompute the mutual information using the correct formula for Gaussian variables.

Wait, mutual information for Gaussian variables is given by:

I(X; Z) = 0.5 * [ log( (1 + ρ_XZ) / ( (1 - ρ_XY)(1 - ρ_YZ) ) ) ) ]

Wait, but in our case, we don't know ρ_XZ. So perhaps the previous approach is incorrect.

Alternatively, perhaps I should compute the mutual information using the formula for the joint Gaussian distribution.

Wait, perhaps I should compute the mutual information using the formula for the conditional entropy, as I did earlier.

Wait, in the earlier approach, I found that I(X; Z) = 0.5 log( (2π)/ (1 - ρ_YZ²) ) ), which for ρ_YZ = 0.9 gives:

I(X; Z) = 0.5 log( (2π) / 0.19 ) ≈ 0.5 log(35.55) ≈ 0.5 * 3.55 ≈ 1.775 bits.

But that's larger than the options given. So perhaps that approach is wrong.

Wait, perhaps I should use the formula for mutual information between two Gaussian variables, given their correlation and another correlation.

Wait, the mutual information between X and Z is given by:

I(X; Z) = 0.5 * [ log( (1 + ρ_XZ) / ( (1 - ρ_XY)(1 - ρ_YZ) ) ) ) ]

But in our case, we don't know ρ_XZ. So perhaps that's the wrong formula.

Alternatively, perhaps the mutual information between X and Z can be calculated using the formula:

I(X; Z) = 0.5 * [ log( (1 + ρ_XZ) / ( (1 - ρ_XY)(1 - ρ_YZ) ) ) ) ]

But again, without knowing ρ_XZ, this approach isn't helpful.

Wait, another approach is to calculate the mutual information using the formula for the joint distribution.

Given that X, Y, Z are Gaussian variables, we can compute the joint distribution, and then compute the mutual information.

But perhaps this is getting too complicated.

Wait, perhaps the problem is designed such that the mutual information can be computed using the formula for Gaussian variables, given their pairwise correlations.

In that case, perhaps the correct formula is:

I(X; Z) = 0.5 * [ log( (1 + ρ_XZ) / ( (1 - ρ_XY)(1 - ρ_YZ) ) ) ) ]

But in our case, we don't know ρ_XZ. So perhaps the problem assumes that X and Z are uncorrelated with Y, but that's not the case here.

Wait, perhaps I should consider that the mutual information between X and Z can be expressed as the mutual information between X and Y plus the mutual information between Y and Z minus the mutual information between X and Y and Z.

But in this case, since X→Y→Z is a Markov chain, the mutual information between X and Z is equal to the mutual information between X and Y plus the mutual information between Y and Z minus the mutual information between X and Z, which would imply that I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z).

But in this case, since X→Y→Z is a Markov chain, I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y; Z).

But since I(X; Y; Z) = I(X; Y) + I(Y; Z) - I(X; Z), we can substitute and get:

I(X; Z) = I(X; Y) + I(Y; Z) - [ I(X; Y) + I(Y; Z) - I(X; Z) ]

Which simplifies to:

I(X; Z) = I(X; Y) + I(Y; Z) - I(X; Y) - I(Y; Z) + I(X; Z)

Which is I(X; Z) = I(X; Z), which is a tautology. So this approach isn't helpful.

Wait, perhaps the correct approach is to use the formula for Gaussian variables:

I(X; Z) = 0.5 * [ log( (1 + ρ_XZ) / ( (1 - ρ_XY)(1 - ρ_YZ) ) ) ]

But since we don't know ρ_XZ, perhaps the problem is designed to have ρ_XZ = ρ_XY * ρ_YZ + (1 - ρ_XY^2) * (1 - ρ_YZ^2) * ρ_XY * ρ_YZ.

Wait, perhaps that's the formula for the conditional correlation in a Markov chain.

Alternatively, perhaps I should compute ρ_XZ using the formula:

ρ_XZ = ρ_XY * ρ_YZ + (1 - ρ_XY^2) * (1 - ρ_YZ^2) * ρ_XY * ρ_YZ

Wait, that might make sense. Let me compute that.

Given ρ_XY = 0.1, ρ_YZ = 0.9.

Compute:

ρ_XY * ρ_YZ = 0.1 * 0.9 = 0.09

(1 - ρ_XY^2) * (1 - ρ_YZ^2) = (1 - 0.01) * (1 - 0.81) = 0.99 * 0.19 ≈ 0.1881

Multiply by ρ_XY * ρ_YZ: 0.1 * 0.9 = 0.09

So total ρ_XZ = 0.09 + 0.1881 * 0.09 ≈ 0.09 + 0.0169 ≈ 0.1069.

Wait, that seems low, but perhaps it's correct.

Now, using the formula for mutual information between X and Z:

I(X; Z) = 0.5 * [ log( (1 + ρ_XZ) / ( (1 - ρ_XY)(1 - ρ_YZ) ) ) ]

Plugging in ρ_XZ = 0.1069, ρ_XY = 0.1, ρ_YZ = 0.9.

Compute numerator: 1 + 0.1069 = 1.1069

Denominator: (1 - 0.1)(1 - 0.9) = 0.9 * 0.1 = 0.09

So (1.1069) / (0.09) ≈ 12.30

So log(12.30) is approximately 2.508.

Multiply by 0.5: 0.5 * 2.508 ≈ 1.254 bits.

So the mutual information is approximately 1.254 bits, which is very close to option B: 1.2.

Therefore, the correct answer is B: 1.2.
</think>

The mutual information between X and Z is calculated using the formula:

\[
I(X; Z) = 0.5 \cdot \log\left(\frac{1 + \rho_{XZ}}{(1 - \rho_{XY})(1 - \rho_{YZ})}\right)
\]

Given the correlations:

\[
\rho_{XY} = 0.1, \quad \rho_{YZ} = 0.9, \quad \rho_{XZ} = \rho_{XY} \cdot \rho_{YZ} + (1 - \rho_{XY}^2)(1 - \rho_{YZ}^2) \cdot \rho_{XY} \cdot \rho_{YZ}
\]

Calculating \(\rho_{XZ}\):

\[
\rho_{XY} \cdot \rho_{YZ} = 0.1 \cdot 0.9 = 0.09
\]
\[
(1 - \rho_{XY}^2)(1 - \rho_{YZ}^2) = (1 - 0.01)(1 - 0.81) = 0.99 \cdot 0.19 \approx 0.1881
\]
\[
\rho_{XY} \cdot \rho_{YZ} = 0.1 \cdot 0.9 = 0.09
\]
\[
\rho_{XZ} = 0.09 + 0.1881 \cdot 0.09 \approx 0.09 + 0.0169 \approx 0.1069
\]

Now, calculate the mutual information:

\[
\frac{1 + \rho_{XZ}}{(1 - \rho_{XY})(1 - \rho_{YZ})} = \frac{1 + 0.1069}{(1 - 0.1)(1 - 0.9)} = \frac{1.1069}{0.9 \cdot 0.1} = \frac{1.1069}{0.09} \approx 12.30
\]

\[
\log(12.30) \approx 2.508
\]

\[
I(X; Z) = 0.5 \cdot 2.508 \approx 1.254 \text{ bits}
\]

The closest option is B: 1.2.

ANSWER: 1.2