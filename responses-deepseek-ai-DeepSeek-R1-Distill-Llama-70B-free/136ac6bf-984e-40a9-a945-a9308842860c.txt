Answer the following multiple choice question, selecting from the answer A through to J. After thinking, reply directly with your answer. 

The last character of your response must be the letter you have chosen.

Question:

Suppose that feedback is used on a binary symmetric channel with parameter $p=0.5$. Each time a $Y$ is received, it becomes the next transmission. Thus $X_1$ is Bern(1/2), $X_2=Y_1$, $X_3=Y_2$, \ldots, X_n=Y_{n-1}. Find $\lim_{n\to\infty} \frac{1}{n} I(X_n;Y_n)$ in bits.

Options:

A: 1.25
B: 1.0
C: 2.0
D: 1.5
E: 0.5
F: 0.75
G: 0.9
H: 0.0
I: 0.25
J: 0.1

Answer:
    <think>
<think>
Okay, so I've got this problem about information theory and binary symmetric channels. Hmm, I'm a bit rusty on this, but let me try to think through it.

The question says that feedback is used on a binary symmetric channel with parameter p=0.5. Oh wait, so the channel flips the bit with probability 0.5. That's a pretty noisy channel. Each time a Y is received, it becomes the next transmission. So X_1 is Bernoulli(1/2), which means it's a random bit with equal probability for 0 and 1. Then X_2 is Y_1, X_3 is Y_2, and so on until X_n equals Y_{n-1}.

The task is to find the limit as n approaches infinity of (1/n) times the mutual information I(X_n; Y_n) in bits. The options are given from A to J, with E being 0.5 and H being 0.0, among others.

Wait, mutual information I(X;Y) tells us how much information X gives us about Y. For a binary symmetric channel (BSC), the mutual information is usually calculated as I = H(Y) - H(Y|X), where H is the entropy.

But in this problem, the setup is a bit different because of the feedback. Let me try to model this. So each X_i is determined by the previous Y_{i-1}. Since the channel is symmetric with p=0.5, each Y_i is a noisy version of X_i.

Let me think about what happens as n increases. Since each X is just the previous Y, which is a noisy version of X from the previous step, maybe there's a feedback loop here that could stabilize the information over time.

Wait, let's consider the process step by step. X_1 is a Bernoulli(1/2) random variable, so it's equally likely 0 or 1. Then Y_1 is X_1 transmitted over the BSC with p=0.5. So Y_1 will be X_1 with probability 0.5, and flipped with 0.5 probability. So if X_1 is 0, Y_1 has a 50% chance of being 0 or 1. The same applies if X_1 is 1.

Then X_2 is Y_1. So X_2 is just whatever was received after the first transmission. Then X_2 is sent over the same channel, resulting in Y_2, which again is X_2 with 50% error chance. So X_3 is Y_2, and so on.

Wait a second, so each X_i is Y_{i-1}, which is a possibly flipped version of X_{i-1}. So the entire process is a Markov chain. Let me model this as a two-state system, where each state is the value of X_i.

But wait, since each X_i is Y_{i-1}, and Y_{i-1} is dependent on X_{i-1}, perhaps the process is such that each X_i is equal to X_{i-1} with probability 0.5, and flipped with probability 0.5. Because when you send X_{i-1}, Y_{i-1} is X_{i-1} with probability 0.5, and flipped with 0.5. So X_i is Y_{i-1} = X_{i-1} with 0.5, and 1 - X_{i-1} with 0.5. So the transition between X_{i} and X_{i-1} is a Bernoulli process with flipping probability 0.5.

Wait, but that's interesting. Because then the process of X_i is a Markov chain with transitions: if current state is 0, next state (X_i) is 0 with 0.5 and 1 with 0.5. Same if current state is 1. So the transition matrix is:

From 0:
To 0: 0.5
To 1: 0.5

From 1:
To 0: 0.5
To 1: 0.5

So this is a Markov chain with each state equally likely transitioning to either state, regardless of the current state. Now, what is the stationary distribution of this chain? Well, since the transition probabilities are the same and symmetric, the stationary distribution is uniform: each state (0 and 1) has probability 0.5.

So as n becomes large, X_n tends to be Bernoulli(0.5). Similarly, Y_n is X_n transmitted over BSC(p=0.5), so Y_n is also Bernoulli(0.5), but dependent on X_n.

Wait, but what's the mutual information I(X_n; Y_n) as n approaches infinity?

In the standard BSC with p=0.5, the mutual information is I = H(Y) - H(Y|X). H(Y) is 1 since Y is Bernoulli(0.5). H(Y|X) is the conditional entropy, which for BSC is H(p) = -p log p - (1-p) log(1-p). Here, p=0.5, so H(Y|X) is 1. Therefore, I = 1 - 1 = 0. So the mutual information would be zero.

But wait, that can't be right because the problem is about the limit of (1/n) I(X_n; Y_n). Wait no, wait. Wait, the question is asking about I(X_n; Y_n), not I(X;Y) in general. But as n approaches infinity, X_n is Bernoulli(0.5), and Y_n is a BSC(p=0.5) output given X_n. So I(X_n; Y_n) is the same as the mutual information of a BSC with p=0.5, which is zero, because H(Y|X)=1 and H(Y)=1, so I=0.

Wait that can't be right. Wait, no, because for BSC with p=0.5, the capacity is zero because the channel is useless. Because when p=0.5, the output Y gives no information about X, since it's equally likely to be 0 or 1 regardless of X.

Wait, but in this problem, the question is about the limit of (1/n) I(X_n; Y_n). Wait, I(X_n; Y_n) is the mutual information between X_n and Y_n. But if as n approaches infinity, X_n is Bernoulli(0.5), and Y_n is determined by X_n through a BSC(p=0.5), which makes Y_n also Bernoulli(0.5), independent of X_n. Because for each X_n bit, Y_n is 0 with probability 0.5 and 1 with 0.5, regardless of X_n. So X_n and Y_n are independent, making mutual information zero.

So the limit would be zero, which is option H. But wait, let me think again.

Wait, but the setup is that each X_{i} is Y_{i-1}, so perhaps the process has some sort of dependency that affects the mutual information over time.

Wait, but if the process is such that each X_i is a function of Y_{i-1}, which is a function of X_{i-1}, and so on, then perhaps each X_n is a function of all previous Y's, but in the limit, as n becomes large, X_n approaches a Bernoulli(0.5) distribution. So Y_n is a BSC(p=0.5) applied to X_n, which is Bernoulli(0.5). Therefore, Y_n is also Bernoulli(0.5), and independent of X_n.

Wait, if X_n and Y_n are independent, then their mutual information is zero. So I(X_n; Y_n) is zero in the limit. Therefore, the limit of (1/n) * 0 is zero. So the answer would be H: 0.0.

But wait, another approach: perhaps I should compute I(X_n; Y_n) for each n and then take the average.

Wait, but let me think: For each n, X_n is Y_{n-1}, which is a function of X_{n-1}, which is Y_{n-2}, etc. So each X_n is a function of the previous channel outputs.

But in the limit, as n approaches infinity, the process reaches a steady state where X_n is Bernoulli(0.5), and Y_n is Bernoulli(0.5) independent of X_n, so I(X_n; Y_n)=0.

Alternatively, perhaps I'm missing something. Let me consider that each Y_i is a function of X_i, which is a function of Y_{i-1}, etc.

Wait, but perhaps the mutual information I(X_n; Y_n) is the same as the mutual information of the BSC with p=0.5, which is zero. So (1/n)*0 is zero, so the limit is zero, which is option H.

Wait, but another thought: maybe the information accumulates in some way because of the feedback. Wait, no, because each time you're just sending the received bit again, so it's not adding any new information beyond what's in the previous bit. So over time, the mutual information per step becomes zero, and thus the average over n steps would also be zero.

Alternatively, perhaps I should model the process as a Markov chain and compute the mutual information.

Wait, the mutual information rate for a Markov chain is given by the limit as n approaches infinity of (1/n) sum_{i=1}^n I(X_i; Y_i), but in this case, each term I(X_i; Y_i) would be the same as the mutual information for the BSC with p=0.5, which is zero for each i. Therefore, the average would be zero.

Wait, but each I(X_i; Y_i) is zero, so the sum is zero, and the average is zero. So the limit is zero.

Alternatively, perhaps the mutual information isn't zero for each i. Let me compute I(X_n; Y_n) for general n.

Wait, for each i, X_i is Y_{i-1}, which is a function of X_{i-1}, and so on. So X_i depends on X_{i-1}, which depends on X_{i-2}, etc., forming a Markov chain.

But when considering I(X_n; Y_n), it's the mutual information between X_n and Y_n. Let's compute this for a general n.

Wait, X_n is Y_{n-1}, and Y_n is the channel output when X_n is sent. So Y_n is a function of X_n (i.e., Y_{n-1}) and the channel noise.

Wait, but X_n is Y_{n-1}, which is the output of the channel for X_{n-1}. So X_n is a function of X_{n-1} and the noise from the previous step.

But in any case, Y_n is the output when X_n is sent through the BSC(p=0.5). So given X_n, Y_n is X_n with probability 0.5 and 1 - X_n with 0.5.

So H(Y_n | X_n) = H(0.5) = 1 bit.

H(Y_n) is the entropy of Y_n. Since X_n is Bernoulli(0.5), Y_n is also Bernoulli(0.5), because the channel output is equally likely to be 0 or 1 regardless of X_n. So H(Y_n) = 1 bit.

Thus, I(X_n; Y_n) = H(Y_n) - H(Y_n | X_n) = 1 - 1 = 0 bits.

So for each n, I(X_n; Y_n) is zero. So the average over n terms would also be zero.

Therefore, the limit is zero, which is option H.
</think>

The problem involves a binary symmetric channel (BSC) with parameter \( p = 0.5 \), where feedback is used such that each transmitted bit \( X_i \) is equal to the previously received bit \( Y_{i-1} \). The goal is to find the limit as \( n \to \infty \) of \( \frac{1}{n} I(X_n; Y_n) \) in bits.

### Key Analysis:
1. **Channel Properties**: The BSC with \( p = 0.5 \) is a highly noisy channel where the output \( Y \) is equally likely to be the same as the input \( X \) or flipped, independent of \( X \). This makes \( Y \) independent of \( X \) in steady state.

2. **Process Description**:
   - \( X_1 \) is Bernoulli(1/2), meaning it is equally likely to be 0 or 1.
   - \( X_2 = Y_1 \), \( X_3 = Y_2 \), etc., so each \( X_i \) is determined by the previous channel output \( Y_{i-1} \).
   - The feedback creates a Markov chain where \( X_n \) depends on \( Y_{n-1} \), which in turn depends on \( X_{n-1} \), and so on.

3. **Stationary Distribution**:
   - As \( n \to \infty \), the process reaches a steady state where \( X_n \) becomes Bernoulli(1/2). This is because the symmetric feedback and channel noise cause \( X_n \) to lose any dependence on initial conditions.
   - Similarly, \( Y_n \), being the output of a BSC with \( p = 0.5 \), is also Bernoulli(1/2).

4. **Mutual Information**:
   - Mutual information \( I(X_n; Y_n) \) is defined as \( H(Y_n) - H(Y_n | X_n) \), where \( H \) denotes entropy.
   - \( H(Y_n) = 1 \) bit because \( Y_n \) is Bernoulli(1/2).
   - \( H(Y_n | X_n) = 1 \) bit because the channel introduces maximum uncertainty due to \( p = 0.5 \).
   - Thus, \( I(X_n; Y_n) = H(Y_n) - H(Y_n | X_n) = 1 - 1 = 0 \) bits.

5. **Limit of \( \frac{1}{n} I(X_n; Y_n) \)**:
   - Since \( I(X_n; Y_n) = 0 \) for all \( n \), the average \( \frac{1}{n} I(X_n; Y_n) \) is also zero.
   - Therefore, the limit as \( n \to \infty \) is \( 0 \).

### Conclusion:
The limit of \( \frac{1}{n} I(X_n; Y_n) \) as \( n \to \infty \) is \( 0 \) bits. This corresponds to option **H**.

H