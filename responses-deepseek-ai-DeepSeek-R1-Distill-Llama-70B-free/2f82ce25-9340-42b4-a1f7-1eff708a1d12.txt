Answer the following multiple choice question, selecting from the answer A through to J. After thinking, reply directly with your answer. 

The last character of your response must be the letter you have chosen.

Question:

Why might a researcher use a variable ratio of reinforcement rather than a fixed ratio?

Options:

A: Fixed ratio schedules of reinforcements are more time intensive.
B: Fixed ratio schedules of reinforcement allow for easier data collection.
C: Variable ratio schedules of reinforcement are less effective but easier to implement.
D: Variable ratio schedules of reinforcements avoid problems such as generalization and the Premack principle.
E: Variable ratio schedules of reinforcements are less susceptible to habituation.
F: Fixed ratio schedules of reinforcement are less predictable, thus more effective.
G: Fixed ratio schedules of reinforcement are more resistant to extinction than variable schedules.
H: Variable ratio schedules of reinforcement are more resistant to extinction than fixed schedules.
I: Variable ratio schedules of reinforcements produce results more quickly.

Answer:
    <think>
<think>
Okay, I'm trying to figure out why a researcher would choose a variable ratio of reinforcement over a fixed ratio. Hmm, I remember that operant conditioning has different schedules of reinforcement. Let me think about each one.

So, fixed ratio means the reinforcement is given after a set number of responses. Like, every 10th time someone does something, they get a reward. It's consistent. But I think this can lead to predictable behavior. People might work really hard up to that 10th and then take a break, knowing the reward is coming.

Variable ratio is different because the number of responses needed isn't fixed. It's unpredictable. So you might get a reward after 5 responses, then 2, then 15, and so on. This unpredictability probably makes the behavior more consistent because the person doesn't know when the next reward is coming. They might keep responding at a steady rate without slacking off.

Wait, what about extinction? Extinction is when the behavior stops because the reinforcement stops. I think variable ratio schedules make behaviors more resistant to extinction. Because if you suddenly stop reinforcing, the person (or animal) might keep trying for longer, expecting that the next reward could be just around the corner. Whereas with fixed ratio, if the reinforcement stops, they might notice the pattern and give up faster since they're used to a predictable reward.

So the question is asking why a researcher would choose variable ratio. The answer should relate to resistance to extinction. Because variable ratio makes the behavior stickier, harder to extinguish. So looking at the options, which one says that?

Looking through the options:

Option H says variable ratio schedules are more resistant to extinction than fixed. That sounds right.

Option G says the opposite, so that's incorrect. So the correct answer should be H.
</think>

The correct answer is H because variable ratio schedules of reinforcement make behaviors more resistant to extinction. This is due to the unpredictability of rewards, which leads to more consistent responding compared to fixed ratio schedules.

Answer: H