Answer the following multiple choice question, selecting from the answer A through to J. After thinking, reply directly with your answer. 

The last character of your response must be the letter you have chosen.

Question:

Why might a researcher use a variable ratio of reinforcement rather than a fixed ratio?

Options:

A: Fixed ratio schedules of reinforcements are more time intensive.
B: Fixed ratio schedules of reinforcement allow for easier data collection.
C: Variable ratio schedules of reinforcement are less effective but easier to implement.
D: Variable ratio schedules of reinforcements avoid problems such as generalization and the Premack principle.
E: Variable ratio schedules of reinforcements are less susceptible to habituation.
F: Fixed ratio schedules of reinforcement are less predictable, thus more effective.
G: Fixed ratio schedules of reinforcement are more resistant to extinction than variable schedules.
H: Variable ratio schedules of reinforcement are more resistant to extinction than fixed schedules.
I: Variable ratio schedules of reinforcements produce results more quickly.

Answer:
    <think>
<think>
Okay, so I've got this question about reinforcement schedules in psychology. Hmm, I remember that reinforcement schedules are part of operant conditioning, right? The question is asking why a researcher would choose a variable ratio over a fixed ratio.

Wait, let me think. Fixed ratio means that the reinforcement is given after a fixed number of responses. Like, every 10th lever press gets a reward. Variable ratio is when it's an unpredictable number, so the reinforcement comes after an average number but varies each time. So like, sometimes it's after 5 presses, sometimes 15, averaging maybe 10.

I think variable ratio schedules are known for creating a high and steady response rate. Oh, right, because the person doesn't know when the next reward is coming, so they keep responding consistently. Also, variable ratio has something to do with resistance to extinction. Extinction is when the behavior stops because the reinforcement stops. So if a variable ratio is used, the behavior is more resistant to extinction. Because with fixed ratio, if you suddenly stop giving rewards, the person might give up faster since they expect a reward after a certain number of actions.

Looking at the options, the question is why a researcher would use variable ratio. The options are A through I.

Option H says variable ratio schedules are more resistant to extinction than fixed. That sounds familiar. Because with variable, the subject doesn't know when the reward is coming, so even if they don't get it immediately, they'll keep trying longer, making the behavior last longer. So in research, if you want a behavior to persist even after you stop reinforcing it, variable ratio is better.

Let me go through the other options to make sure. Option G says fixed schedules are more resistant, which I think is the opposite. So not G. Option E says variable is less susceptible to habituation. Wait, habituation is when a response decreases because the stimulus is repeated. I'm not sure if that's directly related, but the key point here is extinction, not habituation.

Option H seems correct. So the answer should be H.
</think>

The question asks why a researcher might choose a variable ratio of reinforcement over a fixed ratio. Variable ratio schedules provide reinforcement after an unpredictable number of responses, which leads to a high and steady response rate. Additionally, behaviors reinforced under variable ratio schedules are more resistant to extinction compared to fixed ratio schedules. This is because the unpredictability of reinforcement encourages persistence even when rewards are withheld, making variable ratio schedules more effective in sustaining behavior over time.

Analyzing the options:

- **H** correctly states that variable ratio schedules are more resistant to extinction than fixed schedules, aligning with the reasoning above.
- Other options, such as **G** and **E**, either contradict this or address unrelated concepts like habituation.

Thus, the correct answer is **H**.

Answer: H